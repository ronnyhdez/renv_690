---
title: "PCA with tidymodels"
author: "Ronny A. Hern√°ndez Mora"
date: "11/01/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidymodels)
tidymodels_prefer()
library(beans)
library(corrplot)
library(bestNormalize)
library(ggforce)
library(learntidymodels)
```


## Creating test and training data

```{r}
set.seed(1701)
bean_split <- initial_split(beans, strata = class, prop = 3/4)

bean_train <- training(bean_split)
bean_test  <- testing(bean_split)

set.seed(1702)
bean_val <- validation_split(bean_train, strata = class, prop = 4/5)
bean_val$splits[[1]]
```

## Exploring the data

```{r}
tmwr_cols <- colorRampPalette(c("#91CBD765", "#CA225E"))
bean_train %>% 
  select(-class) %>% 
  cor() %>% 
  corrplot(col = tmwr_cols(200), tl.col = "black")
```

## Creating recipe

This dataset contains predictors that are ratios. Given the nature of this
variable, the distribution can be skewed and wreak the havoc on variance
calculations. That's why is better to normalize (enforce a symmetric distribution)
for the predictors

```{r}
bean_rec <-
  # Use the training data from the bean_val split object
  recipe(class ~ ., data = analysis(bean_val$splits[[1]])) %>%
  step_zv(all_numeric_predictors()) %>%
  step_orderNorm(all_numeric_predictors()) %>% 
  step_normalize(all_numeric_predictors())
```

## estimate model with the training data

```{r}
bean_rec_trained <- prep(bean_rec)
bean_rec_trained
```

If we are facing errors we can use:

 - `prep(verbose = TRUE)` this will show the errors in every step
 - `prep(log_changes = TRUE)` will keep the logs for every step
 
## Baking the recipe

`bake()` is like `fit()`, so at this point we have just fit our model, now we
need to predict.

```{r}
bean_validation <- bean_val$splits %>% 
  pluck(1) %>% 
  assessment()

bean_val_processed <- bake(bean_rec_trained, new_data = bean_validation)
```

## Feature extraction techniques

We are going to create a function to plot the data in a scatter plot. After
running the PCA, we will use this function.
```{r}
plot_validation_results <- function(recipe, 
                                    dat = assessment(bean_val$splits[[1]])) {
  recipe %>%
    # Estimate any additional steps
    prep() %>%
    # Process the data (the validation set by default)
    bake(new_data = dat) %>%
    # Create the scatterplot matrix
    ggplot(aes(x = .panel_x, y = .panel_y, col = class, fill = class)) +
    geom_point(alpha = 0.4, size = 0.5) +
    geom_autodensity(alpha = .3) +
    facet_matrix(vars(-class), layer.diag = 2) + 
    scale_color_brewer(palette = "Dark2") + 
    scale_fill_brewer(palette = "Dark2")
}
# alberta_pca %>%
#   step_pca(all_numeric_predictors(), num_comp = 4) %>%
#   plot_validation_results() + 
#   ggtitle("Principal Component Analysis")
```

Now, we are going to add one more step to our recipe to check the variation of
our variables and visualize it with our function:
```{r}
bean_rec_trained %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>%
  plot_validation_results() + 
  ggtitle("Principal Component Analysis")
```

We can observe that the first two principal components are good separating the 
classes.

Now, the PCA components of our data that explain most of the variation in the
predictors are also the predictive of the classes. To get to know what features
are driving each of the components we can do the following:
```{r}
bean_rec_trained %>%
  step_pca(all_numeric_predictors(), num_comp = 4) %>% 
  prep() %>% 
  plot_top_loadings(component_number <= 4, n = 5) + 
  ggtitle("Principal Component Analysis")
```

